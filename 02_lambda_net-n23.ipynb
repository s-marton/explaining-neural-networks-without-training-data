{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation of Î»-Nets for I-Net training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specitication of Experiment Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "###################################################### CONFIG FILE ####################################################################\n",
    "#######################################################################################################################################\n",
    "sleep_time = 0 #minutes\n",
    "\n",
    "config = {\n",
    "    'function_family': {\n",
    "        'maximum_depth': 3,\n",
    "        'beta': 1,\n",
    "        'decision_sparsity': 1,\n",
    "        'fully_grown': True,          \n",
    "        'dt_type': 'vanilla', #'vanilla', 'SDT'\n",
    "    },\n",
    "    'data': {\n",
    "        'number_of_variables': 23, \n",
    "        'num_classes': 2,\n",
    "        'categorical_indices': [],\n",
    "        'random_parameters_distribution': True, \n",
    "        'max_distributions_per_class': 1, # None; 0; int >= 1\n",
    "        'exclude_linearly_seperable': True,\n",
    "        'data_generation_filtering': False,\n",
    "        'fixed_class_probability': False,\n",
    "        'balanced_data': True,\n",
    "        'weighted_data_generation': False,\n",
    "        'shift_distrib': False,\n",
    "        \n",
    "        'function_generation_type': 'distribution' ,\n",
    "        'distribution_list': ['uniform', 'normal', 'gamma', 'beta', 'poisson'],\n",
    "        'objective': 'classification', \n",
    "        \n",
    "        'x_max': 1,\n",
    "        'x_min': 0,\n",
    "        'x_distrib': 'uniform', #'normal', 'uniform',       \n",
    "                \n",
    "        'lambda_dataset_size': 5000, #number of samples per function\n",
    "        'number_of_generated_datasets': 10000,\n",
    "        \n",
    "        'noise_injected_level': 0, \n",
    "        'noise_injected_type': 'flip_percentage', \n",
    "        \n",
    "        'data_noise': 0, #None or float\n",
    "        'distrib_param_max': 5,\n",
    "    }, \n",
    "    'lambda_net': {\n",
    "        'epochs_lambda': 1000,\n",
    "        'early_stopping_lambda': True, \n",
    "        'early_stopping_min_delta_lambda': 1e-3,\n",
    "        'batch_lambda': 64,\n",
    "        'dropout_lambda': 0,\n",
    "        'lambda_network_layers': [128],\n",
    "        'use_batchnorm_lambda': False,\n",
    "        \n",
    "        'optimizer_lambda': 'adam',\n",
    "        'loss_lambda': 'binary_crossentropy', \n",
    "                \n",
    "        'number_initializations_lambda': 1, \n",
    "        \n",
    "        'number_of_trained_lambda_nets': 10000,\n",
    "    },     \n",
    "    'evaluation': {   \n",
    "    },    \n",
    "    \n",
    "    'computation':{\n",
    "        'n_jobs': 50,\n",
    "        'use_gpu': False,\n",
    "        'gpu_numbers': '0',\n",
    "        'RANDOM_SEED': 42,   \n",
    "    }\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "########################################### IMPORT GLOBAL VARIABLES FROM CONFIG #######################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(config['function_family'])\n",
    "globals().update(config['data'])\n",
    "globals().update(config['lambda_net'])\n",
    "globals().update(config['evaluation'])\n",
    "globals().update(config['computation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "from itertools import product       # forms cartesian products\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from more_itertools import random_product \n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt \n",
    "import time\n",
    "import math\n",
    "import operator\n",
    "from functools import reduce\n",
    "\n",
    "from scipy.signal import argrelextrema\n",
    "\n",
    "import sys\n",
    "\n",
    "import shutil\n",
    "\n",
    "import logging\n",
    "logging.getLogger('tensorflow').disabled = True\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.metrics import accuracy_score, log_loss, roc_auc_score, f1_score, mean_absolute_error, r2_score\n",
    "#from similaritymeasures import frechet_dist, area_between_two_curves, dtw\n",
    "from IPython.display import Image\n",
    "\n",
    "from livelossplot import PlotLossesKerasTF\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "#import tensorflow_addons as tfa\n",
    "import random \n",
    "\n",
    "\n",
    "#import keras\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################################\n",
    "###################################################### SET VARIABLES + DESIGN #########################################################\n",
    "#######################################################################################################################################\n",
    "    \n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = gpu_numbers if use_gpu else ''\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true' if use_gpu else ''\n",
    "\n",
    "#os.environ['XLA_FLAGS'] =  '--xla_gpu_cuda_data_dir=/usr/local/cuda-10.1'\n",
    "\n",
    "#os.environ['XLA_FLAGS'] = '--tf_xla_enable_xla_devices'\n",
    "#os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices'\n",
    "\n",
    "os.environ['XLA_FLAGS'] = '--xla_gpu_cuda_data_dir=/usr/local/cuda-11.4' if use_gpu else ''#-10.1' #--xla_gpu_cuda_data_dir=/usr/local/cuda, \n",
    "os.environ['TF_XLA_FLAGS'] = '--tf_xla_auto_jit=2 ,--tf_xla_enable_xla_devices' if use_gpu else ''#'--tf_xla_auto_jit=2' #, --tf_xla_enable_xla_devices\n",
    "\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "if int(tf.__version__[0]) >= 2:\n",
    "    tf.random.set_seed(RANDOM_SEED)\n",
    "else:\n",
    "    tf.set_random_seed(RANDOM_SEED)\n",
    "    \n",
    "    \n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "pd.set_option('display.max_columns', 200)\n",
    "np.set_printoptions(threshold=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.LambdaNet import *\n",
    "from utilities.metrics import *\n",
    "from utilities.utility_functions import *\n",
    "#######################################################################################################################################\n",
    "####################################################### CONFIG ADJUSTMENTS ############################################################\n",
    "#######################################################################################################################################\n",
    "\n",
    "config['lambda_net']['number_of_lambda_weights'] = get_number_of_lambda_net_parameters(config)\n",
    "\n",
    "#######################################################################################################################################\n",
    "################################################## UPDATE VARIABLES ###################################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(config['function_family'])\n",
    "globals().update(config['data'])\n",
    "globals().update(config['lambda_net'])\n",
    "globals().update(config['evaluation'])\n",
    "globals().update(config['computation'])\n",
    "\n",
    "#initialize_utility_functions_config_from_curent_notebook(config)\n",
    "#initialize_LambdaNet_config_from_curent_notebook(config)\n",
    "#initialize_metrics_config_from_curent_notebook(config)\n",
    "\n",
    "#######################################################################################################################################\n",
    "###################################################### PATH + FOLDER CREATION #########################################################\n",
    "#######################################################################################################################################\n",
    "globals().update(generate_paths(config, path_type='lambda_net'))\n",
    "generate_directory_structure()\n",
    "generate_lambda_net_directory(config)\n",
    "\n",
    "#######################################################################################################################################\n",
    "############################################################ SLEEP TIMER ##############################################################\n",
    "#######################################################################################################################################\n",
    "sleep_minutes(sleep_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(path_identifier_lambda_net_data)\n",
    "\n",
    "print(path_identifier_function_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Num XLA-GPUs Available: \", len(tf.config.experimental.list_physical_devices('XLA_GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_network_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T09:46:11.182937Z",
     "start_time": "2021-01-17T09:44:31.797522Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_functions = './data/saved_function_lists/functions_' + path_identifier_function_data + '.csv'\n",
    "function_df = pd.read_csv(path_functions)\n",
    "\n",
    "path_X_data = './data/saved_function_lists/X_data_' + path_identifier_function_data + '.pkl'\n",
    "with open(path_X_data, 'rb') as f:\n",
    "    X_data_list = pickle.load(f)\n",
    "    \n",
    "path_y_data = './data/saved_function_lists/y_data_' + path_identifier_function_data + '.pkl'\n",
    "with open(path_y_data, 'rb') as f:\n",
    "    y_data_list = pickle.load(f)\n",
    "    \n",
    "try:\n",
    "    path_distribution = './data/saved_function_lists/distribution_parameter_list_list_' + path_identifier_function_data + '.pkl'\n",
    "    with open(path_distribution, 'rb') as f:\n",
    "        distribution_parameter_list_list = pickle.load(f)\n",
    "except Exception:\n",
    "    traceback.print_exc()\n",
    "    distribution_parameter_list_list = [[None, None]]*number_of_generated_datasets\n",
    "    \n",
    "if number_of_trained_lambda_nets < number_of_generated_datasets:\n",
    "    function_df = function_df.sample(n=number_of_trained_lambda_nets, random_state=RANDOM_SEED)\n",
    "    random.seed(RANDOM_SEED)\n",
    "    X_data_list = random.sample(X_data_list, number_of_trained_lambda_nets)\n",
    "    random.seed(RANDOM_SEED)\n",
    "    y_data_list = random.sample(y_data_list, number_of_trained_lambda_nets)\n",
    "    random.seed(RANDOM_SEED)\n",
    "    try:\n",
    "        distribution_parameter_list_list = random.sample(distribution_parameter_list_list, number_of_trained_lambda_nets)\n",
    "        random.seed(RANDOM_SEED)\n",
    "    except:\n",
    "        distribution_parameter_list_list = [[None, None]]*number_of_trained_lambda_nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data_list[0][0].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T09:46:12.626401Z",
     "start_time": "2021-01-17T09:46:12.608200Z"
    }
   },
   "outputs": [],
   "source": [
    "X_data_list[0][1].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data_list[0][0].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T09:46:12.636995Z",
     "start_time": "2021-01-17T09:46:12.629349Z"
    }
   },
   "outputs": [],
   "source": [
    "y_data_list[0][1].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = None\n",
    "if plot_tree_from_parameters != 'make_classification':\n",
    "    plot = plot_tree_from_parameters(y_data_list[0][0].values, config)\n",
    "plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inject_noise(y_data, noise=0.0, noise_dist='normal', seed=42):\n",
    "    \"\"\"\n",
    "    Returns a polynomial injected with specified noise type and amount. \n",
    "    Inputs a clean polynomial array with x-data and y-data and modifies the y-data according to the noise settings.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    #Calculate range of function values of input polynomial\n",
    "    range_values_fv = np.ptp(y_data[1].values)\n",
    "    \n",
    "    #Selection of noise type\n",
    "    #Available types: normal, uniform, A, B, C\n",
    "    if noise_dist=='normal':\n",
    "        noise_sample=noise*np.random.normal(loc=0, scale=1.0,size=y_data[1].shape[0])\n",
    "    elif noise_dist=='uniform':\n",
    "        noise_sample=noise*np.random.uniform(low=0, high=1.0,size=y_data[1].shape[0])\n",
    "    elif noise_dist=='normal_range':\n",
    "        #Normal distribution adjusted to function value range\n",
    "        noise_sample=(noise*range_values_fv)*np.random.normal(loc=0, scale=1.0,size=y_data[1].shape[0])\n",
    "    elif noise_dist=='uniform_range':\n",
    "        noise_sample=(noise*range_values_fv)*np.random.normal(loc=0, scale=1.0,size=y_data[1].shape[0])\n",
    "    elif noise_dist=='flip_percentage':\n",
    "        idx = np.random.choice(y_data[1].shape[0], int(y_data[1].shape[0]*noise), replace=False)\n",
    "        y_data[1].iloc[idx] = (y_data[1].iloc[idx] + 1) % 2 \n",
    "        return y_data\n",
    "    else:\n",
    "        #no noise\n",
    "        noise_sample = np.zeros(y_data[1].shape[0])\n",
    "                               \n",
    "    noise_sample=noise_sample.reshape(-1,1)\n",
    "    \n",
    "    y_data[1]=y_data[1]+noise_sample\n",
    "    \n",
    "    return np.clip(y_data, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if noise_injected_level > 0:\n",
    "    parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='loky')\n",
    "    y_data_list = parallel(delayed(inject_noise)(y_data=y_data_list[i],\n",
    "                                                 noise=noise_injected_level,\n",
    "                                                 noise_dist=noise_injected_type, \n",
    "                                                 seed=RANDOM_SEED+i) for i in range(len(y_data_list)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data_list[0][0].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data_list[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data_list[0][1].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T17:06:36.919643Z",
     "start_time": "2020-09-16T17:06:36.912904Z"
    }
   },
   "source": [
    "## Lambda Network Training + Weigh/Bias saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rand_index = np.random.randint(number_of_trained_lambda_nets)\n",
    "\n",
    "#rand_tree = generate_decision_tree_from_array(y_data_list[rand_index][0].values, config)\n",
    "#rand_tree.plot_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%autoreload 2\n",
    "random_network = train_lambda_net(config=config,\n",
    "                                  lambda_index=rand_index, \n",
    "                                  X_data_with_function=X_data_list[rand_index], \n",
    "                                  y_data_with_function=y_data_list[rand_index], \n",
    "                                  distribution_parameter_list = distribution_parameter_list_list[rand_index][1],\n",
    "                                  callbacks=[PlotLossesKerasTF()], \n",
    "                                  return_model=True,\n",
    "                                  printing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "colors_list = ['green','blue','yellow','cyan','magenta','pink']\n",
    "\n",
    "if config['data']['number_of_variables'] > 4:\n",
    "    fig,ax = plt.subplots(nrows=np.ceil(config['data']['number_of_variables']*2/4).astype(int), ncols=4,figsize=(20,15))\n",
    "else:\n",
    "    fig,ax = plt.subplots(nrows=np.ceil(config['data']['number_of_variables']*2/2).astype(int), ncols=2,figsize=(20,15))\n",
    "\n",
    "for axis_1 in ax:\n",
    "    for axis_2 in axis_1:\n",
    "        axis_2.set_xlim([0, 1])                          \n",
    "\n",
    "plot_index = 0\n",
    "\n",
    "for i in range(X_data_list[rand_index][1].values.shape[1]):\n",
    "    colors = colors_list[i%6]\n",
    "\n",
    "    x = X_data_list[rand_index][1].values[:,i][np.where(random_network['model'].predict(X_data_list[rand_index][1].values).ravel()<=0.5)]\n",
    "    plt.subplot(np.ceil(config['data']['number_of_variables']*2/4).astype(int), 4,plot_index+1)\n",
    "    plt.hist(x,bins=[i/10 for i in range(11)],color=colors)\n",
    "    #plt.title(list(distribution_parameter.keys())[0] + ' Class 0' )\n",
    "    plot_index += 1\n",
    "\n",
    "    x = X_data_list[rand_index][1].values[:,i][np.where(random_network['model'].predict(X_data_list[rand_index][1].values).ravel()>0.5)]\n",
    "    plt.subplot(np.ceil(config['data']['number_of_variables']*2/4).astype(int),4,plot_index+1)\n",
    "    plt.hist(x,bins=[i/10 for i in range(11)],color=colors)\n",
    "    #plt.title(list(distribution_parameter.keys())[0] + ' Class 1' )\n",
    "    plot_index += 1\n",
    "\n",
    "fig.subplots_adjust(hspace=0.4,wspace=.3) \n",
    "plt.suptitle('Sampling from Various Distributions',fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "distribution_parameter_list_list[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%load_ext autoreload\n",
    "#%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parallel = Parallel(n_jobs=n_jobs, verbose=3, backend='loky') #loky\n",
    "clf_list = parallel(delayed(train_lambda_net)(config=config,\n",
    "                                              lambda_index=index, \n",
    "                                              X_data_with_function=X_data, \n",
    "                                              y_data_with_function=y_data,\n",
    "                                              distribution_parameter_list=distribution_parameter_list[1],\n",
    "                                              return_history=True, \n",
    "                                              printing=True) for index, (X_data, y_data, distribution_parameter_list) in enumerate(zip(X_data_list, y_data_list, distribution_parameter_list_list)))  \n",
    "del parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_network['model'].get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "random_network['model'].summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and Plot Lambda-Model History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-17T09:44:26.874Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss_list_total = []\n",
    "metric_list_total = []\n",
    "\n",
    "val_loss_list_total = []\n",
    "val_metric_list_total = []\n",
    "\n",
    "index_list = []\n",
    "\n",
    "\n",
    "max_training_epochs = 0\n",
    "for _, entry in tqdm(enumerate(clf_list)):\n",
    "    history = entry['history']\n",
    "    \n",
    "    current_training_epochs = len(history[list(history.keys())[0]])\n",
    "    max_training_epochs = max(max_training_epochs, current_training_epochs)\n",
    "\n",
    "\n",
    "for _, entry in tqdm(enumerate(clf_list)):\n",
    "    history = entry['history']\n",
    "    index = entry['index']\n",
    "    \n",
    "    current_training_epochs = len(history[list(history.keys())[0]])\n",
    "    \n",
    "    loss_list = np.full(max_training_epochs, np.nan)\n",
    "    metric_list = np.full(max_training_epochs, np.nan)\n",
    "    val_loss_list = np.full(max_training_epochs, np.nan)\n",
    "    val_metric_list = np.full(max_training_epochs, np.nan) \n",
    "\n",
    "    for i in range(current_training_epochs):  \n",
    "        loss_list[i] = history[list(history.keys())[0]][i]\n",
    "        metric_list[i] = history[list(history.keys())[1]][i]\n",
    "        val_loss_list[i] = history[list(history.keys())[len(history.keys())//2]][i]\n",
    "        val_metric_list[i] = history[list(history.keys())[len(history.keys())//2+1]][i]\n",
    "    \n",
    "    index_list.append([index])\n",
    "    loss_list_total.append(loss_list)\n",
    "    metric_list_total.append(metric_list)\n",
    "    val_loss_list_total.append(val_loss_list)\n",
    "    val_metric_list_total.append(val_metric_list)\n",
    "\n",
    "loss_df = pd.DataFrame(data=np.hstack([index_list, loss_list_total]), columns=flatten_list(['index', [list(history.keys())[0] + '_epoch_' + str(i+1) for i in range(max_training_epochs)]]))\n",
    "#loss_df['index'] = loss_df['index'].astype(int)\n",
    "metric_df = pd.DataFrame(data=np.hstack([index_list, metric_list_total]), columns=flatten_list(['index', [list(history.keys())[1] + '_epoch_' + str(i+1) for i in range(max_training_epochs)]]))\n",
    "#metric_df['index'] = metric_df['index'].astype(int)\n",
    "val_loss_df = pd.DataFrame(data=np.hstack([index_list, val_loss_list_total]), columns=flatten_list(['index', [list(history.keys())[len(history.keys())//2] + '_epoch_' + str(i+1) for i in range(max_training_epochs)]]))\n",
    "#val_loss_df['index'] = val_loss_df['index'].astype(int)\n",
    "val_metric_df = pd.DataFrame(data=np.hstack([index_list, val_metric_list_total]), columns=flatten_list(['index', [list(history.keys())[len(history.keys())//2+1] + '_epoch_' + str(i+1) for i in range(max_training_epochs)]])) \n",
    "#val_metric_df['index'] = val_metric_df['index'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-17T09:44:26.875Z"
    }
   },
   "outputs": [],
   "source": [
    "path_loss = './data/weights/weights_' + path_identifier_lambda_net_data + '/history_' + list(history.keys())[0] + '.txt'\n",
    "path_metric = './data/weights/weights_' + path_identifier_lambda_net_data + '/history_' + list(history.keys())[1] + '.txt'\n",
    "path_val_loss = './data/weights/weights_' + path_identifier_lambda_net_data + '/history_' + list(history.keys())[len(history.keys())//2] + '.txt'\n",
    "path_val_metric = './data/weights/weights_' + path_identifier_lambda_net_data + '/history_' + list(history.keys())[len(history.keys())//2+1] + '.txt'\n",
    "\n",
    "loss_df.to_csv(path_loss, index=None, sep=',')\n",
    "metric_df.to_csv(path_metric, index=None, sep=',')\n",
    "val_loss_df.to_csv(path_val_loss, index=None, sep=',')\n",
    "val_metric_df.to_csv(path_val_metric, index=None, sep=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-17T09:44:26.876Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-17T09:44:26.877Z"
    }
   },
   "outputs": [],
   "source": [
    "val_loss_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-17T09:44:26.879Z"
    }
   },
   "outputs": [],
   "source": [
    "metric_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-17T09:44:26.880Z"
    }
   },
   "outputs": [],
   "source": [
    "val_metric_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-17T09:44:26.881Z"
    }
   },
   "outputs": [],
   "source": [
    "# summarize history for metric\n",
    "path = './data/results/weights_' + path_identifier_lambda_net_data + '/' + list(history.keys())[1] + '.png'\n",
    "\n",
    "adjustment_threshold_metric = 10#100\n",
    "    \n",
    "metric_df_adjusted = metric_df.copy(deep=True).iloc[:,1:]\n",
    "if adjustment_threshold_metric > 0:\n",
    "    metric_df_adjusted[metric_df_adjusted.columns] = np.where(metric_df_adjusted[metric_df_adjusted.columns] > adjustment_threshold_metric, adjustment_threshold_metric, metric_df_adjusted[metric_df_adjusted.columns])\n",
    "    \n",
    "val_metric_df_adjusted = val_metric_df.copy(deep=True).iloc[:,1:]\n",
    "if adjustment_threshold_metric > 0:\n",
    "    val_metric_df_adjusted[val_metric_df_adjusted.columns] = np.where(val_metric_df_adjusted[val_metric_df_adjusted.columns] > adjustment_threshold_metric, adjustment_threshold_metric, val_metric_df_adjusted[val_metric_df_adjusted.columns])\n",
    "\n",
    "    \n",
    "plt.plot(metric_df_adjusted.describe().loc['mean'].values)\n",
    "plt.plot(val_metric_df_adjusted.describe().loc['mean'].values)\n",
    "plt.title('model ' + list(history.keys())[1])\n",
    "plt.ylabel(list(history.keys())[1])\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='upper left')\n",
    "plt.savefig(path)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-17T09:44:26.883Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "path = './data/results/weights_' + path_identifier_lambda_net_data + '/' + list(history.keys())[0] + '.png'\n",
    "\n",
    "adjustment_threshold_loss = 0#10000\n",
    "    \n",
    "loss_df_adjusted = loss_df.copy(deep=True).iloc[:,1:]\n",
    "\n",
    "if adjustment_threshold_loss > 0:\n",
    "    loss_df_adjusted[loss_df_adjusted.columns] = np.where(loss_df_adjusted[loss_df_adjusted.columns] > adjustment_threshold_loss, adjustment_threshold_loss, loss_df_adjusted[loss_df_adjusted.columns])\n",
    "    \n",
    "val_loss_df_adjusted = val_loss_df.copy(deep=True).iloc[:,1:]\n",
    "if adjustment_threshold_loss > 0:\n",
    "    val_loss_df_adjusted[val_loss_df_adjusted.columns] = np.where(val_loss_df_adjusted[val_loss_df_adjusted.columns] > adjustment_threshold_loss, adjustment_threshold_loss, val_loss_df_adjusted[val_loss_df_adjusted.columns])\n",
    "\n",
    "    \n",
    "plt.plot(loss_df_adjusted.describe().loc['mean'].values)\n",
    "plt.plot(val_loss_df_adjusted.describe().loc['mean'].values)\n",
    "plt.title('model ' + list(history.keys())[0])\n",
    "plt.ylabel(list(history.keys())[0])\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='upper left')\n",
    "plt.savefig(path)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if use_gpu:\n",
    "    from numba import cuda \n",
    "    device = cuda.get_current_device()\n",
    "    device.reset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
